{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Trick Comparison\n",
    "In this notebook, we:\n",
    "1. Load our custom diffusion model (trained on color patches).\n",
    "2. Load the official Stable Diffusion v1.5 model from Hugging Face.\n",
    "3. Perform an identity test on both models for a couple of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_test(unet, vae, latents_scaling, real_image, device):\n",
    "    \"\"\"\n",
    "    For an input image:\n",
    "    1. Encode image -> latents (using VAE)\n",
    "    2. Assume t=0 (no noise)\n",
    "    3. Model should predict ~0 noise\n",
    "    Returns the MSE between predicted noise and 0.\n",
    "    \"\"\"\n",
    "    real_image = real_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(real_image).latent_dist.sample()\n",
    "        latents = latents * latents_scaling\n",
    "\n",
    "        t = torch.tensor([0]*latents.shape[0], device=device).long()\n",
    "        predicted_noise = unet(latents, t).sample\n",
    "        mse_value = F.mse_loss(predicted_noise, torch.zeros_like(predicted_noise)).item()\n",
    "\n",
    "    return mse_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lapuerta/miniconda3/envs/cell-diffusion-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "from diffusers.models.unets.unet_2d import UNet2DModel\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "\n",
    "MY_UNET_CKPT = \"../../color_diffusion_checkpoints/v1/unet_epoch_9.pt\"\n",
    "vae_my = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "vae_my.eval()\n",
    "for param in vae_my.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "unet_my = UNet2DModel(\n",
    "    sample_size=64,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(320, 640, 640, 1280),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ")\n",
    "\n",
    "unet_my.load_state_dict(torch.load(MY_UNET_CKPT, map_location=\"cpu\"))\n",
    "unet_my.to(device)\n",
    "unet_my.eval()\n",
    "\n",
    "my_latent_scaling = 0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity MSE for 'black3.png': 0.431787\n",
      "Identity MSE for 'blue3.png': 0.082421\n",
      "Identity MSE for 'green3.jpg': 0.044352\n",
      "Identity MSE for 'red3.jpg': 0.607667\n",
      "Identity MSE for 'yellow3.jpg': 0.182590\n"
     ]
    }
   ],
   "source": [
    "# Suppose we test with a few color images from your dataset\n",
    "test_image_paths = [\n",
    "    \"../../datasets/colors/black/black3.png\",\n",
    "    \"../../datasets/colors/blue/blue3.png\",\n",
    "    \"../../datasets/colors/green/green3.jpg\",\n",
    "    \"../../datasets/colors/red/red3.jpg\",\n",
    "    \"../../datasets/colors/yellow/yellow3.jpg\",\n",
    "]\n",
    "\n",
    "transform_my = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "for img_path in test_image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform_my(img).unsqueeze(0)  # shape [1,3,H,W]\n",
    "    mse = identity_test(unet_my, vae_my, my_latent_scaling, img_tensor, device)\n",
    "    print(f\"Identity MSE for '{os.path.basename(img_path)}': {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "\n",
    "\n",
    "# Load VAE and UNet separately\n",
    "vae_15 = AutoencoderKL.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"vae\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "unet_15 = UNet2DConditionModel.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "vae_15.eval()\n",
    "unet_15.eval()\n",
    "for p in vae_15.parameters():\n",
    "    p.requires_grad_(False)\n",
    "for p in unet_15.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# We'll assume the same scaling factor 0.18215, though it's typically in config too\n",
    "sd15_latent_scaling = 0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD1.5 Identity MSE for 'black3.png': 0.117680\n",
      "SD1.5 Identity MSE for 'blue3.png': 0.042924\n",
      "SD1.5 Identity MSE for 'green3.jpg': 0.034675\n",
      "SD1.5 Identity MSE for 'red3.jpg': 0.087377\n",
      "SD1.5 Identity MSE for 'yellow3.jpg': 0.039821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_15 = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "for img_path in test_image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform_15(img).unsqueeze(0)  # shape [1,3,H,W]\n",
    "\n",
    "    # Because the unet_15 is a UNet2DConditionModel, we can do:\n",
    "    # unet_15(noisy_latents, t, encoder_hidden_states=?) in normal usage\n",
    "    # but for identity test, we'll pass None or an empty 'encoder_hidden_states'.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        latents = vae_15.encode(img_tensor).latent_dist.sample()\n",
    "        latents = latents * sd15_latent_scaling\n",
    "\n",
    "        t = torch.tensor([0]*latents.shape[0], device=device)\n",
    "        # Official SD unet is conditional, so we must pass something for encoder_hidden_states\n",
    "        # For pure identity test, let's pass a dummy zero embedding of the correct shape.\n",
    "        # Typically stable diffusion uses text encoder hidden states shape [batch, max_len, hidden_dim].\n",
    "        # We'll just use zeros with shape [1, 77, 768] if that matches the model config.\n",
    "        # (This is a hack to \"simulate\" no conditioning.)\n",
    "        dummy_embeds = torch.zeros((1, 77, 768), device=device)\n",
    "\n",
    "        pred_noise = unet_15(latents, t, encoder_hidden_states=dummy_embeds).sample\n",
    "        mse_15 = F.mse_loss(pred_noise, torch.zeros_like(pred_noise)).item()\n",
    "\n",
    "    print(f\"SD1.5 Identity MSE for '{os.path.basename(img_path)}': {mse_15:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-diffusion-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
